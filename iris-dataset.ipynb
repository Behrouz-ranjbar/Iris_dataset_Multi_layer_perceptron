{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _**Import Package**_\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# _**Load Dataset**_\n",
    "\n",
    "iris_dataset = load_iris()\n",
    "\n",
    "iris_dataset\n",
    "\n",
    "data = iris_dataset['data']\n",
    "target = iris_dataset['target']\n",
    "\n",
    "# _**Train_Test_Split Data**_\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.3)\n",
    "\n",
    "# _**Data Loader**_\n",
    "\n",
    "class IRISDataset():\n",
    "  def __init__(self, x, y):\n",
    "    self.x = torch.tensor(x, dtype=torch.float32)\n",
    "    self.y = torch.tensor(y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.x[idx], self.y[idx]    \n",
    "\n",
    "iris_dataset_train = IRISDataset(x_train, y_train)\n",
    "iris_dataset_test = IRISDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(iris_dataset_train, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(iris_dataset_test, batch_size=8, shuffle=True)\n",
    "\n",
    "for x, y in train_loader:\n",
    "  print(x, y)\n",
    "\n",
    "# _**Multi Layer Perceptron**_\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MLP, self).__init__()\n",
    "    self.l1 = nn.Linear(4, 10)\n",
    "    self.l2 = nn.Linear(10, 12)\n",
    "    self.l3 = nn.Linear(12, 3)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x1 = self.l1(x)\n",
    "    x1_tanh = torch.tanh(x1)\n",
    "    x2 = self.l2(x1_tanh)\n",
    "    x2_sigmoid = torch.sigmoid(x2)\n",
    "    x3 = self.l3(x2_sigmoid)\n",
    "    out = torch.softmax(x3, dim=1)\n",
    "    return out\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=0.1)\n",
    "\n",
    "# _**Training Loop**_\n",
    "\n",
    "for epoch in range(80):\n",
    "  epoch_loss = []\n",
    "  epoch_acc = []\n",
    "  for x, y in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(x)\n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_score = accuracy_score(y_true=y, y_pred=pred.argmax(dim=1))\n",
    "\n",
    "    epoch_loss.append(loss.item())\n",
    "    epoch_acc.append(acc_score)\n",
    "\n",
    "  print(f\"epoch: {epoch}, loss: {np.mean(epoch_loss)}, accuracy: {np.mean(epoch_acc)}\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  epoch_loss = []\n",
    "  epoch_acc = []\n",
    "  for x, y in test_loader:\n",
    "    pred = model(x)\n",
    "    loss = criterion(pred, y)\n",
    "    epoch_loss.append(loss.item())\n",
    "\n",
    "    acc = accuracy_score(pred.argmax(dim=1), y)\n",
    "    epoch_acc.append(acc)\n",
    "\n",
    "    print(f\"test loss: {np.mean(epoch_loss)}, test accuracy: {np.mean(epoch_acc)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
